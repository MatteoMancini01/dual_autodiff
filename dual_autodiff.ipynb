{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation Using Dual Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "This Jupyter Notebook is designed as a guide to illustrate how to use the dual_autodiff package, a Python package created to perform automatic differentiations using dual numbers. Firstly, a brief introduction on the concepts of dual numbers and automatic differentiations, then some examples on how to use the package. The package contains all trigonometric and hyperbolic functions, furthermore the exponential and logarithmic functions. <span style=\"color:red\">The only dependency in the package is numpy, for more infromation on numpy see [link](https://numpy.org/), make sure all the packages required in dual_autodiff are also installed</span> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO-DO:\n",
    "- KEEP AN EYE ON THE RED TEXTS (NEED MODIFICATION)\n",
    "- add a set up section that expains how to create a virtual enviroment and install the package using `pip install -e .`\n",
    "- ADD ANOTHER CLASS CALLED DERIVATIVE TO COMPUTE THE NUMERICAL DERIVATIVE (maybe not required in numpy you have gradient which computes the derivative, scipy.misc has derivative and more)\n",
    "- ADD A PLOTTING OPTION TO CLASS DUAL SO THAT IT COMPUTES THE DUAL WITH A RANGE OF VALUES INSTEAD OF SIGLE OPINTS, E.G. `x = np.linspace()` as input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "#### Dual Numbers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dual_autodiff import Dual\n",
    "from dual_autodiff import NumDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dual_autodiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dual(real = 2, dual = 1)\n"
     ]
    }
   ],
   "source": [
    "x = Dual(2,1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.4161468365471424"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sin().dual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.9612372705533612\n"
     ]
    }
   ],
   "source": [
    "x = Dual(1.5, 1) # choose 1 for dual so we obtain the derivative\n",
    "f_x  = x.sin().log() + x**2*x.cos() # log(sin(x)) + x**2cos(x)\n",
    "\n",
    "derivative_at_x_real = f_x.dual\n",
    "print(derivative_at_x_real)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.000000000000001\n"
     ]
    }
   ],
   "source": [
    "from dual_autodiff import NumDiff as nd # numerical differentiation\n",
    "\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "y_der = nd(f, 2, 0.1)\n",
    "dy = y_der.first_central()\n",
    "print(dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "385a162aaba14516b46eb0136b23b6e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.1, description='h', max=0.99, min=1e-05, step=0.01), Output()), _domâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TASK 5\n",
    "\n",
    "from ipywidgets import interactive\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_with_parameters(h): \n",
    "\n",
    "    def f(x): \n",
    "        return np.log(np.sin(x)) + x**2*np.cos(x)\n",
    "\n",
    "    def df_true(x): \n",
    "        return 1/np.tan(x) + 2*x*np.cos(x) - x**2*np.sin(x)\n",
    "\n",
    "    # Dual derivative\n",
    "    x_space = np.linspace(1, 2, 100)\n",
    "    x = Dual(x_space, 1) # set dual part equal to 1\n",
    "    f_dual = x.sin().log() + (x**2)*x.cos()\n",
    "    df_dual = f_dual.dual # take the dual part of f to find its derivative at x.real=1.5 \n",
    "\n",
    "# Analytical derivative\n",
    "    true_der = df_true(x.real)\n",
    "\n",
    "    # Calculate f(t) for these parameters\n",
    "    ndf = NumDiff(f, x.real, h) # x=1.5 \n",
    "    ndf_for = ndf.first_forward() # forward difference\n",
    "    ndf_cen = ndf.first_central() # central difference\n",
    "    ndf_bac = ndf.first_backwards() # backward difference\n",
    "\n",
    "    # Plot the function\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x_space, true_der, label='First Order Analytical')\n",
    "    plt.plot(x_space, df_dual, label='Automatic Differentiation', color = 'black', linestyle = ':')\n",
    "    plt.plot(x_space, ndf_cen, label='First Order Numerical Central', color = 'red', linestyle = '--')\n",
    "    plt.plot(x_space, ndf_for, label='First Order Numerical Forward', color = 'purple', linestyle = '--')\n",
    "    plt.plot(x_space, ndf_bac, label='First Order Numerical Backward', color = 'blue', linestyle = '--')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel(r'$\\frac{df}{dx} = cot(x) +  2xcos(x) - x^2sin(x)$')\n",
    "    plt.title('Numerical vs Analytical')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Create interactive widget\n",
    "interactive_plot = interactive(\n",
    "    plot_with_parameters,\n",
    "    h=widgets.FloatSlider(min=10**(-5), max=0.99, step=0.01, value=0.1),\n",
    ")\n",
    "display(interactive_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dual(real = 0, dual = 1)\n",
      "2\n",
      "Dual(real = 1, dual = 3)\n"
     ]
    }
   ],
   "source": [
    "x=Dual(2, 6)\n",
    "y=Dual(4, -2)\n",
    "n=2\n",
    "\n",
    "print(x//y)\n",
    "print((((8 * 4) - (2 * -2)) // (4 ** 2)))\n",
    "print(x//n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dual(real = 3.461624963063523, dual = -7.012115744346404)\n"
     ]
    }
   ],
   "source": [
    "x = Dual(3,1)\n",
    "f_x = (x.tanh().cosh())**2 + x.sinh().sin().exp().tan() + x.log().cos()\n",
    "print(f_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
      "platform linux -- Python 3.12.3, pytest-8.3.3, pluggy-1.5.0\n",
      "rootdir: /root/Document/Python_Script/dual_autodiff\n",
      "configfile: pyproject.toml\n",
      "collected 19 items\n",
      "\n",
      "t_e1326bfc2f7c4f9da696a18f225b0c26.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                    [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m19 passed\u001b[0m\u001b[32m in 0.10s\u001b[0m\u001b[32m ========================================\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExitCode.OK: 0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tests import TestDual\n",
    "from tests import NumDiff\n",
    "import ipytest\n",
    "\n",
    "TestDual, NumDiff\n",
    "ipytest.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Differentiation In 1D\n",
    "\n",
    "Add text here!\n",
    "\n",
    "For a function $f(x)$, whose domain are dual numbers, i.e. $a + \\epsilon b$, where both $a,b \\in \\R$, then\n",
    "$$\n",
    "f(a + \\epsilon b) = f(x)|_{x=a} + \\epsilon b f'(x)|_{x=a}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Differentiation In Higher Dimensions\n",
    "\n",
    "We have seen how automatic differentiation using dual numbers is utilised in one dimension, but can we extend this into higher dimensions?  The answer is yes. One can compute first order partial derivatives with respect to each variable for a high dimensional function. \n",
    "\n",
    "Consider the function $f(\\vec{x})$, where $\\vec{x} = (x_1, x_2,...,x_n)^T$, whose domain $\\vec{a} + \\epsilon \\vec{b}$, where $\\vec{a} = (a_1, a_2,...,a_n)^T$ and $\\vec{b} = (b_1, b_2,...,b_n)^T$, then we can extend the one dimensional model to higher dimensions as follow:\n",
    "$$\n",
    "f(\\vec{a} + \\epsilon \\vec{b}) = f(\\vec{x})|_{\\vec{x}=\\vec{a}} + \\epsilon \\sum^n_{i=1} b_i \\frac{\\partial f(\\vec{x})}{\\partial x_i}|_{\\vec{x} = \\vec{a}}\n",
    "$$\n",
    "\n",
    "If we set all $b_i=1$ then the dual part of $f(\\vec{a} + \\epsilon \\vec{b})$ will be the sum of partial derivatives evaluated at $\\vec{x} = \\vec{a}$, but what if one wants to perform the partial derivative with repsect to a single valriable? The answer this question rather simple, we need to elimina the unwanted partial derivatives.\n",
    "\n",
    "Let us define a new function $f_j(\\vec{x})$, such that:\n",
    "$$\n",
    "f_j(\\vec{a} + \\epsilon \\vec{b}) = f(\\vec{x})|_{\\vec{x}=\\vec{a}} + \\epsilon \\sum^n_{i=1} \\delta_{ij} b_i \\frac{\\partial f(\\vec{x})}{\\partial x_i}|_{\\vec{x} = \\vec{a}}\n",
    "$$ \n",
    "where $\\delta_{ij}$ is the kronecker delta, such that;\n",
    "$$\n",
    "\\delta_{ij} = \n",
    "\\begin{cases} \n",
    "0 & \\text{when } i \\neq j \\\\ \n",
    "1 & \\text{when } i = j\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "And $\\vec{b} = [1,1,1,...,1]^T$, $\\vec{a}$ is the collection of point where the partial derivative of $f(\\vec{x})$ is evaluated at. One can simplify the above equation into:\n",
    "$$\n",
    "f_j(\\vec{a} + \\epsilon \\vec{b}) = f(\\vec{x})|_{\\vec{x}=\\vec{a}} + \\epsilon b_j \\frac{\\partial f(\\vec{x})}{\\partial x_j}|_{\\vec{x} = \\vec{a}}\n",
    "$$\n",
    "\n",
    "The index $j$ is the albitrary choice of the variable we want to differentiate for, e.g. if one wants to compute the partial derivative of $f(\\vec{x})$ with respect to $x_2$ then set $j=2$, here we will see how:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "from dual_autodiff import Dual \n",
    "\n",
    "def f1(x,y): \n",
    "    return x*y\n",
    "\n",
    "dual = Dual(1,0)\n",
    "x,y = 2,3\n",
    "\n",
    "partial = dual.partial_derivative(1,f1,x,y)\n",
    "print(partial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x//n = Dual(real = -2, dual = -3)\n"
     ]
    }
   ],
   "source": [
    "from dual_autodiff import Dual\n",
    "x = Dual(2,1)\n",
    "n = Dual(-1,1)\n",
    "print(f'x//n = {x//n}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial derivaive of f2 with respect to x, at x,y,z=1,-1,2 is -1.4161468365471424\n",
      "Partial derivaive of f2 with respect to y, at x,y,z=1,-1,2 is 1.5403023058681398\n",
      "Partial derivaive of f2 with respect to z, at x,y,z=1,-1,2 is -0.9092974268256817\n"
     ]
    }
   ],
   "source": [
    "def f2(x,y,z):\n",
    "    return x*y + x*z.cos() + y.sin()\n",
    "\n",
    "d = Dual(1,0)\n",
    "\n",
    "x,y,z=Dual(1,0),Dual(-1,0),Dual(2,0)\n",
    "\n",
    "x_partial = d.partial_derivative(0,f2,x,y,z)\n",
    "y_partial = d.partial_derivative(1,f2,x,y,z)\n",
    "z_partial = d.partial_derivative(2,f2,x,y,z)\n",
    "\n",
    "print(f\"Partial derivaive of f2 with respect to x, at x,y,z=1,-1,2 is {x_partial}\")\n",
    "print(f\"Partial derivaive of f2 with respect to y, at x,y,z=1,-1,2 is {y_partial}\")\n",
    "print(f\"Partial derivaive of f2 with respect to z, at x,y,z=1,-1,2 is {z_partial}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "from dual_autodiff import Dual \n",
    "import dual_autodiff as df\n",
    "\n",
    "def f1(x,y): \n",
    "    return x*y\n",
    "\n",
    "x,y = Dual(2,1), Dual(3,0)\n",
    "\n",
    "\n",
    "df_dx = f1(x,y)\n",
    "print(df_dx.dual)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dual_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
