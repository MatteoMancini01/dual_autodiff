{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation Using Dual Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "This Jupyter Notebook is designed as a guide to illustrate how to use the dual_autodiff package, a Python package created to perform automatic differentiations using dual numbers. Firstly, a brief introduction on the concepts of dual numbers and automatic differentiations, then some examples on how to use the package. The package contains all trigonometric and hyperbolic functions, furthermore the exponential and logarithmic functions. <span style=\"color:red\">The only dependency in the package is numpy, for more infromation on numpy see [link](https://numpy.org/), make sure all the packages required in dual_autodiff are also installed</span> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO-DO:\n",
    "- KEEP AN EYE ON THE RED TEXTS (NEED MODIFICATION)\n",
    "- add a set up section that expains how to create a virtual enviroment and install the package using `pip install -e .`\n",
    "- ADD ANOTHER CLASS CALLED DERIVATIVE TO COMPUTE THE NUMERICAL DERIVATIVE (maybe not required in numpy you have gradient which computes the derivative, scipy.misc has derivative and more)\n",
    "- ADD A PLOTTING OPTION TO CLASS DUAL SO THAT IT COMPUTES THE DUAL WITH A RANGE OF VALUES INSTEAD OF SIGLE OPINTS, E.G. `x = np.linspace()` as input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "#### Dual Numbers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dual_autodiff import Dual\n",
    "from dual_autodiff import NumDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dual_autodiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dual(real = 2, dual = 1)\n"
     ]
    }
   ],
   "source": [
    "x = Dual(2,1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.4161468365471424"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sin().dual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.9612372705533612\n"
     ]
    }
   ],
   "source": [
    "x = Dual(1.5, 1) # choose 1 for dual so we obtain the derivative\n",
    "f_x  = x.sin().log() + x**2*x.cos() # log(sin(x)) + x**2cos(x)\n",
    "\n",
    "derivative_at_x_real = f_x.dual\n",
    "print(derivative_at_x_real)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.000000000000001\n"
     ]
    }
   ],
   "source": [
    "from dual_autodiff import NumDiff as nd # numerical differentiation\n",
    "\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "y_der = nd(f, 2, 0.1)\n",
    "dy = y_der.first_central()\n",
    "print(dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Computing the derivative of $f(x)$:\n",
    " $$\n",
    " \\begin{align*}\n",
    " f(x) &= log(sin(x)) + x^2cos(x)\\\\\n",
    " f'(x) &= cot(x) +  2xcos(x) - x^2sin(x)\n",
    " \\end{align*}\n",
    " $$\n",
    " When one defines the dual variable $x$ to determine the derivative of any fucntions $f(x)$ with respect to $x$, should always set the dual part of $x$ equal to $1$. For instance, running the code `x = Dual(1.5, 1)`, will ensure that the derivative of the function $f(x)$ at $Real(x) = 1.5$ is given by the dual part of $f(x)$. Given $x = a + \\epsilon b$, then:\n",
    "  $$\n",
    "\\begin{align*} \n",
    "f(x) = f(a + \\epsilon b) = f(a) + \\epsilon b f'(a)\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "  set $b=1$ thus,\n",
    "$$\n",
    "\\begin{align*} \n",
    "f(x) = f(a + \\epsilon) = f(a) + \\epsilon f'(a)\n",
    "\\end{align*}\n",
    "$$\n",
    "Hence the derivative of $f(x)$ is given by its dual part.\n",
    "\n",
    "Let us code the above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Automatic Differentiation</th>\n",
       "      <th>Forward Differentiation</th>\n",
       "      <th>Central Differentiation</th>\n",
       "      <th>Backward Differentiation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Results</th>\n",
       "      <td>-1.961237</td>\n",
       "      <td>-1.996346</td>\n",
       "      <td>-1.961308</td>\n",
       "      <td>-1.926270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ratio</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.017901</td>\n",
       "      <td>1.000036</td>\n",
       "      <td>0.982171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Absolute Error</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035109</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.034967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Automatic Differentiation  Forward Differentiation  \\\n",
       "Results                         -1.961237                -1.996346   \n",
       "Ratio                            1.000000                 1.017901   \n",
       "Absolute Error                   0.000000                 0.035109   \n",
       "\n",
       "                Central Differentiation  Backward Differentiation  \n",
       "Results                       -1.961308                 -1.926270  \n",
       "Ratio                          1.000036                  0.982171  \n",
       "Absolute Error                 0.000071                  0.034967  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dual_autodiff import Dual\n",
    "from dual_autodiff import NumDiff\n",
    "\n",
    "# f(x)\n",
    "def f(x):\n",
    "    return np.log(np.sin(x)) + x**2*np.cos(x)\n",
    "\n",
    "# f'(x) \n",
    "def df_true(x):\n",
    "    return 1/np.tan(x) + 2*x*np.cos(x) - x**2*np.sin(x)\n",
    "\n",
    "# Dual derivative\n",
    "x = Dual(1.5,1) # set dual part equal to 1\n",
    "f_dual = x.sin().log() + (x**2)*x.cos()\n",
    "df_dual = f_dual.dual # take the dual part of f to find its derivative at x.real=1.5 \n",
    "\n",
    "# Numerical solution\n",
    "h = 0.01 # step size\n",
    "ndf = NumDiff(f, x.real, h) # x=1.5 \n",
    "ndf_for = ndf.first_forward() # forward difference\n",
    "ndf_cen = ndf.first_central() # central difference\n",
    "ndf_bac = ndf.first_backward() # backward difference\n",
    "\n",
    "# Analytical derivative\n",
    "true_der = df_true(x.real)\n",
    "\n",
    "# defining all measures to determine the ratio and the error between analytical and dual/numerical\n",
    "def ratio(n,m):\n",
    "    return abs(n/m)\n",
    "\n",
    "def absolute_error(n,m):\n",
    "    return abs(n-m)\n",
    "\n",
    "# create a dataset to display the impormation collected using pandas\n",
    "\n",
    "information = {\n",
    "    \"Automatic Differentiation\": [df_dual, ratio(df_dual,true_der), absolute_error(df_dual, true_der)],\n",
    "    \"Forward Differentiation\": [ndf_for, ratio(ndf_for,true_der), absolute_error(ndf_for, true_der)],\n",
    "    \"Central Differentiation\": [ndf_cen, ratio(ndf_cen,true_der), absolute_error(ndf_cen, true_der)],\n",
    "    \"Backward Differentiation\":[ndf_bac, ratio(ndf_bac,true_der), absolute_error(ndf_bac, true_der)]\n",
    "}\n",
    "\n",
    "results = pd.DataFrame(information, index=[\"Results\", \"Ratio\", \"Absolute Error\"])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e67fde98b9541bdafaf57452fb6b18e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.1, description='h', max=0.99, min=1e-05, step=0.01), Output()), _dom…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TASK 5\n",
    "\n",
    "from ipywidgets import interactive\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dual_autodiff import NumDiff\n",
    "from dual_autodiff import Dual\n",
    "\n",
    "def plot_with_parameters(h): \n",
    "\n",
    "    def f(x): \n",
    "        return np.log(np.sin(x)) + x**2*np.cos(x)\n",
    "\n",
    "    def df_true(x): \n",
    "        return 1/np.tan(x) + 2*x*np.cos(x) - x**2*np.sin(x)\n",
    "\n",
    "    # Dual derivative\n",
    "    x_space = np.linspace(1, 2, 100)\n",
    "    x = Dual(x_space, 1) # set dual part equal to 1\n",
    "    f_dual = x.sin().log() + (x**2)*x.cos()\n",
    "    df_dual = f_dual.dual # take the dual part of f to find its derivative at x.real=1.5 \n",
    "\n",
    "# Analytical derivative\n",
    "    true_der = df_true(x.real)\n",
    "\n",
    "    # Calculate f(t) for these parameters\n",
    "    ndf = NumDiff(f, x.real, h) # x=1.5 \n",
    "    ndf_for = ndf.first_forward() # forward difference\n",
    "    ndf_cen = ndf.first_central() # central difference\n",
    "    ndf_bac = ndf.first_backward() # backward difference\n",
    "\n",
    "    # Plot the function\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x_space, true_der, label='First Order Analytical')\n",
    "    plt.plot(x_space, df_dual, label='Automatic Differentiation', color = 'black', linestyle = ':')\n",
    "    plt.plot(x_space, ndf_cen, label='First Order Numerical Central', color = 'red', linestyle = '--')\n",
    "    plt.plot(x_space, ndf_for, label='First Order Numerical Forward', color = 'purple', linestyle = '--')\n",
    "    plt.plot(x_space, ndf_bac, label='First Order Numerical Backward', color = 'blue', linestyle = '--')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel(r'$\\frac{df}{dx} = cot(x) +  2xcos(x) - x^2sin(x)$')\n",
    "    plt.title('Numerical vs Analytical')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Create interactive widget\n",
    "interactive_plot = interactive(\n",
    "    plot_with_parameters,\n",
    "    h=widgets.FloatSlider(min=10**(-5), max=0.99, step=0.01, value=0.1),\n",
    ")\n",
    "display(interactive_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dual(real = 0, dual = 1)\n",
      "2\n",
      "Dual(real = 1, dual = 3)\n"
     ]
    }
   ],
   "source": [
    "x=Dual(2, 6)\n",
    "y=Dual(4, -2)\n",
    "n=2\n",
    "\n",
    "print(x//y)\n",
    "print((((8 * 4) - (2 * -2)) // (4 ** 2)))\n",
    "print(x//n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dual(real = 3.461624963063523, dual = -7.012115744346404)\n"
     ]
    }
   ],
   "source": [
    "x = Dual(3,1)\n",
    "f_x = (x.tanh().cosh())**2 + x.sinh().sin().exp().tan() + x.log().cos()\n",
    "print(f_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
      "platform linux -- Python 3.12.3, pytest-8.3.3, pluggy-1.5.0\n",
      "rootdir: /root/Document/Python_Script/dual_autodiff\n",
      "configfile: pyproject.toml\n",
      "collected 19 items\n",
      "\n",
      "t_5f5adac4dc744164aacbb6f5ab274554.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                    [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m19 passed\u001b[0m\u001b[32m in 0.06s\u001b[0m\u001b[32m ========================================\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExitCode.OK: 0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tests import TestDual\n",
    "from tests import NumDiff\n",
    "import ipytest\n",
    "\n",
    "TestDual, NumDiff\n",
    "ipytest.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Differentiation In 1D\n",
    "\n",
    "Add text here!\n",
    "\n",
    "For a function $f(x)$, whose domain are dual numbers, i.e. $a + \\epsilon b$, where both $a,b \\in \\R$, then\n",
    "$$\n",
    "f(a + \\epsilon b) = f(x)|_{x=a} + \\epsilon b f'(x)|_{x=a}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Differentiation In Higher Dimensions\n",
    "\n",
    "We have seen how automatic differentiation using dual numbers is utilised in one dimension, but can we extend this into higher dimensions?  The answer is yes. One can compute first order partial derivatives with respect to each variable for a high dimensional function. \n",
    "\n",
    "Consider the function $f(\\vec{x})$, where $\\vec{x} = (x_1, x_2,...,x_n)^T$, whose domain $\\vec{a} + \\epsilon \\vec{b}$, where $\\vec{a} = (a_1, a_2,...,a_n)^T$ and $\\vec{b} = (b_1, b_2,...,b_n)^T$, then we can extend the one dimensional model to higher dimensions as follow:\n",
    "$$\n",
    "f(\\vec{a} + \\epsilon \\vec{b}) = f(\\vec{x})|_{\\vec{x}=\\vec{a}} + \\epsilon \\sum^n_{i=1} b_i \\frac{\\partial f(\\vec{x})}{\\partial x_i}|_{\\vec{x} = \\vec{a}}\n",
    "$$\n",
    "\n",
    "If we set all $b_i=1$ then the dual part of $f(\\vec{a} + \\epsilon \\vec{b})$ will be the sum of partial derivatives evaluated at $\\vec{x} = \\vec{a}$, but what if one wants to perform the partial derivative with repsect to a single valriable? The answer this question rather simple, we need to elimina the unwanted partial derivatives.\n",
    "\n",
    "Let us define a new function $f_j(\\vec{x})$, such that:\n",
    "$$\n",
    "f_j(\\vec{a} + \\epsilon \\vec{b}) = f(\\vec{x})|_{\\vec{x}=\\vec{a}} + \\epsilon \\sum^n_{i=1} \\delta_{ij} b_i \\frac{\\partial f(\\vec{x})}{\\partial x_i}|_{\\vec{x} = \\vec{a}}\n",
    "$$ \n",
    "where $\\delta_{ij}$ is the kronecker delta, such that;\n",
    "$$\n",
    "\\delta_{ij} = \n",
    "\\begin{cases} \n",
    "0 & \\text{when } i \\neq j \\\\ \n",
    "1 & \\text{when } i = j\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "And $\\vec{b} = [1,1,1,...,1]^T$, $\\vec{a}$ is the collection of point where the partial derivative of $f(\\vec{x})$ is evaluated at. One can simplify the above equation into:\n",
    "$$\n",
    "f_j(\\vec{a} + \\epsilon \\vec{b}) = f(\\vec{x})|_{\\vec{x}=\\vec{a}} + \\epsilon b_j \\frac{\\partial f(\\vec{x})}{\\partial x_j}|_{\\vec{x} = \\vec{a}}\n",
    "$$\n",
    "\n",
    "The index $j$ is the albitrary choice of the variable we want to differentiate for, e.g. if one wants to compute the partial derivative of $f(\\vec{x})$ with respect to $x_2$ then set $j=2$, here we will see how:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "from dual_autodiff import Dual \n",
    "\n",
    "def f1(x,y): \n",
    "    return x*y\n",
    "\n",
    "dual = Dual(1,0)\n",
    "x,y = 2,3\n",
    "\n",
    "partial = dual.partial_derivative(1,f1,x,y)\n",
    "print(partial)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sech{a}$ $sina$ $e^{a}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanh(x) = Dual(real = 0.9640275800758169, dual = 0.4239049491189868)\n"
     ]
    }
   ],
   "source": [
    "from dual_autodiff import Dual\n",
    "x = Dual(2,6)\n",
    "exp_x = x.tanh()\n",
    "print(f'tanh(x) = {exp_x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial derivaive of f with respect to x, at x,y,z=1,-1,2 is -1.4161468365471424\n",
      "Partial derivaive of f with respect to y, at x,y,z=1,-1,2 is 1.5403023058681398\n",
      "Partial derivaive of f with respect to z, at x,y,z=1,-1,2 is -0.9092974268256817\n"
     ]
    }
   ],
   "source": [
    "def f2(x,y,z):\n",
    "    return x*y + x*z.cos() + y.sin()\n",
    "\n",
    "d = Dual(0,0)\n",
    "\n",
    "x,y,z=Dual(1,0),Dual(-1,0),Dual(2,0)\n",
    "\n",
    "x_partial = d.partial_derivative(0,f2,x,y,z)\n",
    "y_partial = d.partial_derivative(1,f2,x,y,z)\n",
    "z_partial = d.partial_derivative(2,f2,x,y,z)\n",
    "\n",
    "print(f\"Partial derivaive of f with respect to x, at x,y,z=1,-1,2 is {x_partial}\")\n",
    "print(f\"Partial derivaive of f with respect to y, at x,y,z=1,-1,2 is {y_partial}\")\n",
    "print(f\"Partial derivaive of f with respect to z, at x,y,z=1,-1,2 is {z_partial}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "from dual_autodiff import Dual \n",
    "import dual_autodiff as df\n",
    "\n",
    "def f1(x,y): \n",
    "    return x*y\n",
    "\n",
    "x,y = Dual(2,1), Dual(3,0)\n",
    "\n",
    "\n",
    "df_dx = f1(x,y)\n",
    "print(df_dx.dual)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward difference of f(x) at the point x0 = 2, with stepsize h = 0.01, is 0.5987105016421079\n"
     ]
    }
   ],
   "source": [
    "from dual_autodiff import NumDiff\n",
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    return x + np.exp(x-x**2)\n",
    "x0 = 2\n",
    "h = 0.01\n",
    "derivative = NumDiff(f,x0,h)\n",
    "result = derivative.first_forward()\n",
    "print(f'Forward difference of f(x) at the point x0 = {x0}, with stepsize h = {h}, is {result}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward difference of f(x) at the point x0 = 2, with stepsize h = 0.01, is 0.5892372009922209\n"
     ]
    }
   ],
   "source": [
    "result = derivative.first_backward()\n",
    "print(f'Backward difference of f(x) at the point x0 = {x0}, with stepsize h = {h}, is {result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Central difference of f(x) at the point x0 = 2, with stepsize h = 0.01, is 0.5939738513171644\n"
     ]
    }
   ],
   "source": [
    "result = derivative.first_central()\n",
    "print(f'Central difference of f(x) at the point x0 = {x0}, with stepsize h = {h}, is {result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second derivative of f(x) at the point x0 = 2, with stepsize h = 0.01, is 0.9473300649887051\n",
      "need to update\n"
     ]
    }
   ],
   "source": [
    "result = derivative.second_order()\n",
    "print(f'Second derivative of f(x) at the point x0 = {x0}, with stepsize h = {h}, is {result}')\n",
    "print('need to update')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dual(real = -1020, dual = -7)\n"
     ]
    }
   ],
   "source": [
    "from dual_autodiff import Dual\n",
    "\n",
    "x, y, z, m = Dual(2,1), Dual(-10,2), Dual(-1,-1), Dual(999,0)\n",
    "\n",
    "print(x*y - m + z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial derivaive of f with respect to x, at x,y,z=1,-1,2 is -1.4161468365471424\n",
      "Partial derivaive of f with respect to y, at x,y,z=1,-1,2 is 1.5403023058681398\n",
      "Partial derivaive of f with respect to z, at x,y,z=1,-1,2 is -0.9092974268256817\n"
     ]
    }
   ],
   "source": [
    "from src.dual_autodiff_x import DualX\n",
    "\n",
    "def f2(x,y,z):\n",
    "    return x*y + x*z.cos() + y.sin()\n",
    "\n",
    "d = DualX(0,0)\n",
    "\n",
    "x,y,z=DualX(1,0),DualX(-1,0),DualX(2,0)\n",
    "\n",
    "x_partial = d.partial_derivative(0,f2,x,y,z)\n",
    "y_partial = d.partial_derivative(1,f2,x,y,z)\n",
    "z_partial = d.partial_derivative(2,f2,x,y,z)\n",
    "\n",
    "print(f\"Partial derivaive of f with respect to x, at x,y,z=1,-1,2 is {x_partial}\")\n",
    "print(f\"Partial derivaive of f with respect to y, at x,y,z=1,-1,2 is {y_partial}\")\n",
    "print(f\"Partial derivaive of f with respect to z, at x,y,z=1,-1,2 is {z_partial}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from src.dual_autodiff_x import DualX\n",
    "x = DualX(2,1)\n",
    "print(-x.dual)\n",
    "\n",
    "from dual_autodiff import Dual\n",
    "y = Dual(2,1)\n",
    "print(y.dual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dual(a,b):\n",
    "    return Dual(a,b)\n",
    "def test_dual_x(a,b):\n",
    "    return DualX(a,b)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146 ns ± 3.36 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n",
      "90.8 ns ± 2.43 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "%timeit test_dual(2,1)\n",
    "%timeit test_dual_x(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dual_autodiff_x import NumDiffX as NDX \n",
    "from dual_autodiff import NumDiff as ND \n",
    "\n",
    "def non_cythonized(f, x, h):\n",
    "    der=ND(f,x,h)\n",
    "    return der.first_central()\n",
    "\n",
    "def cythonized(f, x, h):\n",
    "    der=NDX(f,x,h)\n",
    "    return der.first_central()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494 ns ± 13.6 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n",
      "320 ns ± 10.7 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "%timeit non_cythonized(f,2,0.1)\n",
    "%timeit cythonized(f,2,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "Now try yourself. \n",
    "\n",
    "Complete the following sets of exercises, solutions are provided "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dual_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
